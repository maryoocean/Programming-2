{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "import html\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем ссылки, которые ведут на сраницы с новостями на сайте этой газеты.\n",
    "\n",
    "\n",
    "def get_urls():\n",
    "    url = 'http://mk.tula.ru/news/%s/?PAGEN_1=%s'\n",
    "    topics = ['health', 'culture', 'society', 'incedent', 'sport',\n",
    "              'business', 'oboronka']\n",
    "    k = 1\n",
    "    i = 1\n",
    "    links_list = []\n",
    "    for t in topics:\n",
    "        i = 1\n",
    "        k = 1\n",
    "        # В разделе \"Оборонка\" всего 6 страниц выдачи со стьатями.\n",
    "        # Исключив этот раздел, k можно поставить больше.\n",
    "        # Я выкачивала с 4-х страниц каждого раздела - 840 статей.\n",
    "        # for k in range(1, 7):\n",
    "        for k in range(1, 5):\n",
    "            i = 1\n",
    "            while url and i < 2:\n",
    "                u = url % (t, k)\n",
    "            # print(u)\n",
    "                w = urlopen(u, timeout=5)\n",
    "                h = w.read()\n",
    "                s = BeautifulSoup(h, 'html5lib')\n",
    "                i += 1\n",
    "                if True:\n",
    "                    links = s.find_all('a')\n",
    "                    for l in links:\n",
    "                        l.get('href')\n",
    "                        if l.get('href').startswith('/news/n/'):\n",
    "                            if l.get('href') not in links_list:\n",
    "                                links_list.append(l.get('href'))\n",
    "    return links_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаёем CSV-таблицу и вносим в неё заголовки столбцов\n",
    "\n",
    "\n",
    "def csv_file():\n",
    "    if not os.path.exists('newspaper/'):\n",
    "        os.makedirs('newspaper')\n",
    "    p_csv = 'newspaper/CSV-table.csv'\n",
    "    headers = 'path\\\\tаuthor\\\\theader\\\\tcreated\\\\tsphere\\\\ttopic\\\\tstyle\\\\t \\\n",
    "               audience_age\\\\taudience_level\\\\taudience_size\\\\tsource\\\\t \\\n",
    "               publication\\\\t publ_year\\\\tmedium\\\\tcountry\\\\tregion\\\\tlanguage'\n",
    "    with open(p_csv, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(headers + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Этой функцией мы будем считать количество статей в папке,\n",
    "# чтобы присваивать им соответствующий номер\n",
    "\n",
    "\n",
    "def num_files(path, a, b):\n",
    "    list_dir = []\n",
    "    list_dir = os.listdir(path)\n",
    "    num = 0\n",
    "    for file in list_dir:\n",
    "        if file.endswith(a) or file.endswith(b):\n",
    "            num += 1\n",
    "    return num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлекаем из html-кода страницы метаднные и текст статьи\n",
    "\n",
    "\n",
    "def csv_data(page_html):\n",
    "    regAuthor = re.compile('<p><span>Автор:</span>&nbsp;(.*?)</p>',\n",
    "                           flags=re.DOTALL)\n",
    "    authors = regAuthor.findall(page_html)\n",
    "    if len(authors) == 0:\n",
    "        au = 'None'\n",
    "    else:\n",
    "        au = authors[0]\n",
    "    regTitle = re.compile('<title>(.*?)</title>')\n",
    "    title = regTitle.findall(page_html)[0]\n",
    "    regDate = re.compile('<div class=\"data_news\">.*?<span>(.*?)</span>',\n",
    "                         flags=re.DOTALL)\n",
    "    date_time = regDate.findall(page_html)[0]\n",
    "    date = date_time.split()[0]  # отделяем дату от времени\n",
    "    month = date.split('.')[1]  # берём месяц\n",
    "    year = date.split('.')[2]  # берём год\n",
    "    regCat = re.compile('<div class=\"poster_content_img\">.*?<span>(.\\\\D*?)</span>.*?<img',\n",
    "                        flags=re.DOTALL)\n",
    "    category = regCat.findall(page_html)\n",
    "    if len(category) == 0:\n",
    "        cat = 'None'\n",
    "    else:\n",
    "        cat = category[0]\n",
    "    regHTMLt = re.compile('<div style=\"clear:right; \"></div>(.*?)<div',\n",
    "                          flags=re.DOTALL)\n",
    "    HTMLt = regHTMLt.findall(page_html)\n",
    "    return au, title, date, month, year, cat, HTMLt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём папки и присваиваем файлу со статьёй имя с нужным номером\n",
    "\n",
    "\n",
    "def create_dirs(month, year):\n",
    "    if not os.path.exists('newspaper/plain/%s/%s/' % (year, month)):\n",
    "        os.makedirs('newspaper/plain/%s/%s' % (year, month))\n",
    "    p = 'newspaper/plain/%s/%s/' % (year, month)\n",
    "    n = num_files(p, '.txt', '.xml') + 1\n",
    "    filename = 'статья%s.txt' % n\n",
    "    my_path = p + filename\n",
    "    if not os.path.exists('newspaper/mystem-plain/%s/%s/' % (year, month)):\n",
    "        os.makedirs('newspaper/mystem-plain/%s/%s' % (year, month))\n",
    "    if not os.path.exists('newspaper/mystem-xml/%s/%s/' % (year, month)):\n",
    "        os.makedirs('newspaper/mystem-xml/%s/%s' % (year, month))\n",
    "    return my_path, filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пишем в файл пустой текст, размечаем его с помощью Mystem.\n",
    "# Добавляем метаданные в csv-таблицу.\n",
    "# Переписываем файл с пустым текстом, добавляя в него строки с метаданными.\n",
    "# Наверное, это можно было бы разделить на отдельны функции.\n",
    "\n",
    "\n",
    "def create_files(my_path, au, title, date, month, year, cat, HTMLt,\n",
    "                 pageUrl, filename):\n",
    "    # Печатаем в файл текст статьи\n",
    "    with open(my_path, 'w', encoding=\"utf-8\") as f:\n",
    "        for element in HTMLt:\n",
    "            regTag = re.compile('<.*?>')\n",
    "            element_cl = regTag.sub('', element)\n",
    "            f.write( html.unescape(element_cl) )\n",
    "        f.close()\n",
    "    # print(my_path)\n",
    "    # Добавляем метаданные в CSV-таблицу\n",
    "    row = '%s\\\\t%s\\\\t%s\\\\t%s\\\\tпублицистика\\\\t%s\\\\tнейтральный\\\\tн-возраст\\\\t \\\n",
    "            н-уровень\\\\tрегиональная\\\\t%s\\\\tМолодой коммунар\\\\t%s\\\\t \\\n",
    "            газета\\\\tРоссия\\\\tТула\\\\tru'\n",
    "    p_csv = 'newspaper/CSV-table.csv'\n",
    "    with open(p_csv, 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(row % (my_path, au, title, date, cat, pageUrl, year) + '\\n')\n",
    "        f.close\n",
    "    # Mystem\n",
    "    filename_xml = filename.split('.')[0] + '.xml'\n",
    "    os.system(\"/applications/mystem -lcgid --eng-gr \\\n",
    "                newspaper/plain/%s/%s/%s newspaper/mystem-plain/%s/%s/%s\" %\n",
    "              (year, month, filename, year, month, filename))\n",
    "    os.system(\"/applications/mystem -lcgid --eng-gr --format xml \\\n",
    "                newspaper/plain/%s/%s/%s newspaper/mystem-xml/%s/%s/%s\" %\n",
    "              (year, month, filename, year, month, filename_xml))\n",
    "    # Переписываем первый файл, добавляя строки с метаданными.\n",
    "    with open(my_path, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write('@au ' + au + '\\n')\n",
    "        f.write('@ti ' + title + '\\n')\n",
    "        f.write('@da ' + date + '\\n')\n",
    "        f.write('@topic ' + cat + '\\n')\n",
    "        f.write('@url ' + pageUrl + '\\n')\n",
    "        for element in HTMLt:\n",
    "            regTag = re.compile('<.*?>')\n",
    "            element_cl = regTag.sub('', element)\n",
    "            f.write( html.unescape(element_cl) )\n",
    "        f.close()\n",
    "    with open('newspaper/texts.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        for element in HTMLt:\n",
    "            regTag = re.compile('<.*?>')\n",
    "            element_cl = regTag.sub('', element)\n",
    "            f.write( html.unescape(element_cl) )\n",
    "        f.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собственно функция, извлекающая html-код всей страницы\n",
    "# (потом извлеченный код используется в функциях выше)\n",
    "\n",
    "\n",
    "def download_page(pageUrl):\n",
    "    try:\n",
    "        page = urllib.request.urlopen(pageUrl)\n",
    "        text = page.read().decode('utf-8')\n",
    "    except:\n",
    "        print('Error at', pageUrl)\n",
    "        text = ''\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применяем все вышенаписанные функции для каждой полученной ссылке\n",
    "\n",
    "\n",
    "def news_pages(links_list):\n",
    "    commonUrl = 'http://mk.tula.ru'\n",
    "    for link in links_list:\n",
    "        pageUrl = commonUrl + link\n",
    "        page_html = str(download_page(pageUrl))\n",
    "        au_csv, title_csv, date_csv, month_csv, year_csv, \\\n",
    "        cat_csv, HTMLtext = csv_data(page_html)\n",
    "        path_csv, f_name = create_dirs(month_csv, year_csv)\n",
    "        words_number = create_files(path_csv, au_csv, title_csv, date_csv, month_csv,\n",
    "                     year_csv, cat_csv, HTMLtext, pageUrl, f_name)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаю работать.\n",
      "Всё готово.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print('Начинаю работать.')\n",
    "    csv_file()\n",
    "    all_links = get_urls()\n",
    "    news_pages(all_links)\n",
    "    print('Всё готово.')\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
